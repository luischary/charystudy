## NLP -> MoE


### 2024 Yuan 2.0-M32
- **https://arxiv.org/pdf/2211.12588**
### 2023 Mixture-of-Experts Meets Instruction Tuning
- **2023-05-07**
- https://arxiv.org/pdf/2305.14705
### 2023 Towards MoE Deployment_ Mitigating Inefficiencies_in Mixture-of-Expert (MoE) Inference
- **2023-06-18**
- https://arxiv.org/pdf/2303.06182
### 2024 Mixture-of-Depths
- **2024-02-04**
- https://arxiv.org/pdf/2404.02258
### 202407 Mixture of A Million Experts
- **2024-04-07**
- https://arxiv.org/pdf/2407.04153
### 2024 Camelidae-8x34B
- **2024-08-01**
- https://arxiv.org/pdf/2401.02731
### 2024 Mixtral
- **2024-08-01**
- https://arxiv.org/pdf/2401.04088
### 202409 GRIN_ GRadient-INformed MoE
- **2024-09-18**
- https://arxiv.org/pdf/2409.12136
### 2024 DeepSeekMoE
- **2024-11-01**
- https://arxiv.org/pdf/2401.06066
### 2024 Mixing Expert LLMs into a Mixture-of-Experts LLM
- **2024-12-03**
- https://arxiv.org/pdf/2403.07816