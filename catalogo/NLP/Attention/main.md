## NLP -> Attention


### 2020 Fast autoregres-_sive transformers with linear attention
- **2020-08-31**
- https://arxiv.org/pdf/2006.16236
### Fast Autoregressive Transformers with Linear Attention
- **2020-08-31**
- https://arxiv.org/pdf/2006.16236
### Flash Attention
- **2022-06-23**
- https://arxiv.org/pdf/2205.14135
### 2021 SELF-ATTENTION DOES NOT NEED O(n_2_) MEMORY
- **2022-10-10**
- https://arxiv.org/pdf/2112.05682
### 2023 GQA_ Training Generalized Multi-Query Transformer Models from_Multi-Head Checkpoints
- **2023-05-22**
- https://arxiv.org/pdf/2305.13245
### 2023 TransNormer
- **2023-07-27**
- https://arxiv.org/pdf/2307.14995
### 2023 Blockwise Parallel Transformer_for Large Context Models
- **2023-08-28**
- https://arxiv.org/pdf/2305.1937
### 2023 HyperAttention
- **2023-11-10**
- https://arxiv.org/pdf/2310.05869
### 2023 Ring Attention
- **2023-11-27**
- https://arxiv.org/pdf/2310.01889
### 2024 BurstAttention
- **2024-03-14**
- https://arxiv.org/pdf/2403.09347
### 2024 Lightning Attention-2
- **2024-09-01**
- https://arxiv.org/pdf/2401.04658