## NLP -> LongContext


### 2023 SLED (long document)
- **https://arxiv.org/pdf/**
### Efficient Classification of Long Documents Using Transformers
- **https://arxiv.org/pdf/**
### Revisiting Transformer-based Models for Long Document Classification
- **https://arxiv.org/pdf/**
### 2019 Sparse Transformer
- **2019-04-23**
- https://arxiv.org/pdf/1904.10509
### HIERARCHICAL TRANSFORMERS FOR LONG DOCUMENT CLASSIFICATION
- **2019-10-23**
- https://arxiv.org/pdf/1910.10781
### 2022 LongT5
- **2022-03-05**
- https://arxiv.org/pdf/2112.07916
### CLASSIFICATION OF LONG SEQUENTIAL DATA USING_CIRCULAR DILATED CONVOLUTIONAL NEURAL NETWORKS
- **2022-06-01**
- https://arxiv.org/pdf/2201.02143
### An Exploration of Hierarchical Attention Transformers_for Efficient Long Document Classification
- **2022-11-10**
- https://arxiv.org/pdf/2210.05529
### 2023 Focused Transformer
- **2023-06-07**
- https://arxiv.org/pdf/2307.0317
### 2023 COLT5
- **2023-10-24**
- https://arxiv.org/pdf/2303.09752
### 2023 Cache Transformers
- **2023-12-20**
- https://arxiv.org/pdf/2312.12742
### 202407 Summary of a Haystack
- **2024-01-07**
- https://arxiv.org/pdf/2407.0137
### 2024 LongAlign
- **2024-01-31**
- https://arxiv.org/pdf/2401.18058
### 2024 LLM Maybe LongLM
- **2024-02-01**
- https://arxiv.org/pdf/2401.01325
### 2024 Transformers Can Achieve Length_Generalization But Not Robustly
- **2024-02-14**
- https://arxiv.org/pdf/2402.09371
### 2024 Data Engineering for Scaling Language Models to 128K Context
- **2024-02-15**
- https://arxiv.org/pdf/2402.10171
### 2024 In Search of Needles in a 10M Haystack(1)
- **2024-02-16**
- https://arxiv.org/pdf/2402.1079
### 2024 In Search of Needles in a 10M Haystack
- **2024-02-16**
- https://arxiv.org/pdf/2402.1079
### 2024 In Search of Needles in a 11M Haystack
- **2024-02-21**
- https://arxiv.org/pdf/2402.1079
### 2024 TransformerFAM
- **2024-04-14**
- https://arxiv.org/pdf/2404.09173
### 202408 Dolphin
- **2024-08-28**
- https://arxiv.org/pdf/2408.15518
### 2024 Infinite Context Transformers 
- **2024-10-04**
- https://arxiv.org/pdf/2404.07143