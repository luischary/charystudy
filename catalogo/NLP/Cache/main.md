## NLP -> Cache


### 2024 Layer-Condensed KV Cache
- **2024-05-17**
- https://arxiv.org/pdf/2405.10637
### LazyLLM_ DYNAMIC TOKEN PRUNING FOR EFFICIENT_LONG CONTEXT LLM INFERENCE
- **2024-07-19**
- https://arxiv.org/pdf/2407.14057