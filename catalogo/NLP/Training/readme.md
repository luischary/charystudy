## NLP -> Training



### Efficient Training of Language Models to Fill in the Middle

**2022-07-28**

https://arxiv.org/pdf/2207.14255

This paper presents a method for training autoregressive language models to perform text in-filling by transforming training data, specifically by moving a span of text from the middle of a document to its end. The authors demonstrate that this approach, termed Fill-in-the-Middle (FIM), does not compromise the model's left-to-right generative capabilities, as evidenced by perplexity and sampling evaluations. Key contributions include the establishment of the FIM-for-free property, which allows models to learn in-filling without degrading autoregressive performance, and the provision of best practices for FIM training through extensive hyperparameter ablations. The authors also introduce new in-filling benchmarks and highlight the inefficiency of learning FIM through fine-tuning compared to pretraining. Overall, the paper advocates for the adoption of FIM training in future autoregressive language models.

---

### 202408 Exploring Impact of Code in Pre-training

**2024-08-20**

https://arxiv.org/pdf/2408.10914

---