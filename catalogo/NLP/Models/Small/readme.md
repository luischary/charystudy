## NLP -> Models -> Small



### MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT

**2024-02-26**

https://arxiv.org/pdf/2402.16840

This paper introduces MobiLlama, a Small Language Model (SLM) with 0.5 billion parameters designed for resource-constrained devices. It challenges the trend of larger models by employing a shared feed-forward network (FFN) design across transformer blocks, significantly reducing training costs while maintaining accuracy. MobiLlama is fully transparent, providing access to the training data pipeline, code, model weights, and over 300 checkpoints. The model outperforms existing SLMs of similar size on nine benchmarks, demonstrating its efficiency and effectiveness for on-device processing, privacy, and sustainability.

---

### BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text

**2024-03-27**

https://arxiv.org/pdf/2403.18421

BioMedLM is a 2.7 billion parameter autoregressive language model specifically trained on PubMed abstracts and articles. It aims to provide a smaller, more efficient alternative to larger models like GPT-4 and Med-PaLM 2, which are costly and require extensive computational resources. BioMedLM demonstrates competitive performance on various biomedical NLP tasks, achieving notable scores on multiple-choice question-answering datasets such as MedMCQA and MMLU Medical Genetics. The model's design allows for fine-tuning on specific biomedical tasks while ensuring data privacy and transparency regarding its training data. The authors emphasize the potential of smaller, domain-specific models to enhance biomedical research and healthcare applications.

---

### 202407 MobileLLM

**2024-06-27**

https://arxiv.org/pdf/2402.14905

---