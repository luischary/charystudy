## NLP -> Learning



### Language Models are Unsupervised Multitask Learners

This paper demonstrates that large language models, specifically GPT-2, can perform various natural language processing tasks in a zero-shot setting without explicit supervision. Trained on a diverse dataset called WebText, GPT-2 achieves competitive performance on tasks like reading comprehension, translation, and summarization, often surpassing baseline systems. The authors argue that the model's capacity is crucial for successful task transfer and that it learns to perform tasks from natural language sequences. The findings suggest a promising direction for developing general-purpose language processing systems that can learn from unstructured data.

---

### Professor Forcing: A New Algorithm for Training Recurrent Networks

**2016-10-27**

https://arxiv.org/pdf/1610.09038

This paper introduces Professor Forcing, a novel training algorithm for recurrent neural networks (RNNs) that aims to align the network's behavior during training (teacher forcing) with its behavior during sampling (free-running). By employing adversarial domain adaptation, Professor Forcing encourages the RNN to produce similar dynamics in both modes, thereby improving long-term sequence generation. The authors demonstrate the effectiveness of Professor Forcing across various tasks, including language modeling, handwriting generation, and image generation, showing improvements in test likelihood and sample quality. The method acts as a regularizer, enhances the model's ability to capture long-term dependencies, and reduces the divergence between hidden state distributions during training and sampling. The paper also discusses trade-offs with existing methods like Scheduled Sampling.

---

### 2022 Scaling Language Models_ Methods, Analysis_& Insights from Training Gopher

**2022-01-21**

https://arxiv.org/pdf/2112.11446

---

### 2022 Chinchilla

**2022-03-29**

https://arxiv.org/pdf/2203.15556

---

### Teaching Small Language Models to Reason

**2022-12-19**

https://arxiv.org/pdf/arXiv:2212.08410v2

This paper investigates the transfer of reasoning capabilities from large language models (LLMs) to smaller models through knowledge distillation. The authors propose a method where a student model is fine-tuned on chain of thought (CoT) outputs generated by larger teacher models like PaLM-540B and GPT-3. The experiments demonstrate significant improvements in task performance across arithmetic, commonsense, and symbolic reasoning datasets, with notable accuracy increases on the GSM8K dataset. The authors recommend specific strategies for effective knowledge distillation, including generating CoT data with provided solutions and focusing on single tasks. The findings suggest that even small amounts of CoT examples can enhance the reasoning abilities of smaller models, highlighting the potential for efficient training with limited data.

---

### Small-scale proxies for large-scale Transformer training instabilities

**2023-09-25**

https://arxiv.org/pdf/2309.14322

This paper investigates training instabilities in large Transformer models, which have been reported at scale but not easily reproducible due to resource constraints. The authors focus on two specific instabilities: the growth of logits in attention layers and divergence of output logits from log probabilities. They demonstrate that these instabilities can also occur in smaller models when trained at high learning rates. The study introduces 'learning rate sensitivity' as a metric to assess the impact of learning rate variations on final loss. The authors explore various interventions, such as qk-layernorm and z-loss regularization, which effectively mitigate these instabilities across different model scales. Additionally, they predict instabilities by analyzing scaling behaviors of model characteristics like activation and gradient norms, providing insights for future research on training stability without requiring extensive computational resources.

---

### Towards Optimal Learning of Language Models

**2024-02-27**

https://arxiv.org/pdf/2402.17759

This paper presents a theory for optimizing the learning of language models (LMs) by viewing LM training as a process of lossless data compression. The authors propose an objective to maximize the data compression ratio, leading to a theorem called the Learning Law, which states that all training examples should contribute equally in the optimal learning process. The theory is validated through experiments on linear classification and real-world language modeling tasks, demonstrating significant speedups in training (5.50× for Perceptron and 2.41× for Transformer) and improvements in scaling law coefficients. The findings emphasize the potential for designing practical learning acceleration methods in LM training.

---

### Stealing Part of a Production Language Model

**2024-03-11**

https://arxiv.org/pdf/2403.06634v1

This paper introduces a novel model-stealing attack that successfully extracts significant information from black-box production language models, specifically targeting the embedding projection layer of transformer models like OpenAI's ChatGPT and Google's PaLM-2. The authors demonstrate that their attack can recover the entire projection matrix of certain models for a minimal cost, revealing hidden dimensions of 1024 and 2048 for OpenAI's ada and babbage models, respectively. The attack operates by making targeted queries to the model's API, exploiting the low-rank nature of the final layer. The authors also discuss the implications of their findings, potential defenses against such attacks, and the need for further research in this area. Their work highlights the vulnerabilities in current API designs and the ease with which adversaries can extract sensitive model information.

---