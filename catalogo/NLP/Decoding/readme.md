## NLP -> Decoding



### Recurrent Drafter for Fast Speculative Decoding in Large Language Models

**2024-03-14**

https://arxiv.org/pdf/2403.09919

This paper presents a novel approach called Recurrent Drafter (ReDrafter) aimed at enhancing the efficiency of speculative decoding in large language models (LLMs). The method integrates a single, lightweight draft head with a recurrent dependency design, simplifying the inference process while maintaining effectiveness. By leveraging beam search to filter low-quality candidate sequences and employing a dynamic tree attention mechanism, ReDrafter reduces computational overhead compared to existing methods like Medusa. Empirical results demonstrate that ReDrafter outperforms Medusa in terms of accuracy and inference speed, making it a practical solution for efficient LLM serving.

---