## NLP -> Agents -> Memory



### WALKING DOWN THE MEMORY MAZE: BEYOND CONTEXT LIMIT THROUGH INTERACTIVE READING

**2023-10-08**

https://arxiv.org/pdf/2310.05029

The paper introduces MEMWALKER, a novel method for enhancing long-text understanding by treating large language models (LLMs) as interactive agents. MEMWALKER constructs a memory tree from long texts, allowing the model to navigate and selectively read relevant segments based on user queries. This approach overcomes limitations of fixed context windows and traditional methods like recurrence and retrieval. The authors demonstrate that MEMWALKER outperforms existing models on long-context question answering tasks, improves explainability by detailing reasoning steps, and effectively utilizes working memory during navigation. The study highlights the importance of reasoning capabilities in LLMs for successful navigation and suggests future research directions for further enhancing interactive reading.

---