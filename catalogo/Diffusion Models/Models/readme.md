## Diffusion Models -> Models



### Hierarchical Text-Conditional Image Generation with CLIP Latents

**2022-04-13**

https://arxiv.org/pdf/2204.06125

This paper presents unCLIP, a two-stage model for text-conditional image generation that utilizes CLIP embeddings to enhance image diversity while maintaining photorealism. The model consists of a prior that generates CLIP image embeddings from text captions and a decoder that produces images based on these embeddings. The authors demonstrate that this approach allows for variations in generated images while preserving semantic and stylistic elements. They compare their method with existing models like DALL-E and GLIDE, showing that unCLIP achieves comparable quality with greater diversity. The study also explores image manipulations and the structure of the CLIP latent space, revealing insights into the model's capabilities and limitations.

---

### OmniGen: Unified Image Generation

**2024-09-17**

https://arxiv.org/pdf/2409.11340v1

OmniGen is a novel unified diffusion model for image generation that integrates various tasks such as text-to-image generation, image editing, and classic computer vision tasks within a single framework. It simplifies the architecture by eliminating the need for additional modules like ControlNet, allowing for a more user-friendly experience. The model is trained on a large-scale dataset called X2I, enabling effective knowledge transfer across tasks and demonstrating competitive performance on benchmarks. OmniGen also exhibits reasoning capabilities and supports in-context learning, making it a versatile tool for diverse image generation applications.

---