## Quantization



### OneBit: Towards Extremely Low-bit Large Language Models

**2024-02-17**

https://arxiv.org/pdf/2402.11295

This paper presents OneBit, a novel framework for quantizing large language models (LLMs) to 1-bit weight matrices, significantly reducing storage and computational requirements. The authors introduce a quantization-aware training (QAT) approach that includes a unique 1-bit parameter representation and an effective parameter initialization method based on matrix decomposition. Experimental results demonstrate that OneBit achieves at least 83% of the performance of non-quantized models while maintaining robust training processes. The study highlights the challenges of existing quantization methods at low bit-widths and showcases the advantages of the proposed method across various model sizes, indicating its potential for efficient deployment of LLMs.

---